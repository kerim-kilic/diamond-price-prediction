---
title: "Diamond price prediction"
author: "Kerim KiliÃ§"
subtitle: Supervised Machined Learning
output:
  html_document:
    df_print: paged
    toc: true
    toc_depth: 2
    number_sections: true
    toc_float: true
---

# Libraries

The following three libraries are used in this R notebook.

```{r setup, message=FALSE}
library(tidymodels)
library(GGally)
library(gridExtra)
```

# Dataset descriptive analytics

Glimpse into the variables of the *diamonds* dataset.

```{r}
diamonds %>% glimpse()
```

Take a smaller sample from dataset to reduce time to train the model.
```{r}
diamonds_sample <- diamonds %>% slice_sample(prop = 0.2)
```

ggpairs plot of all the variables and their correlation in the dataset. Set *eval* to *TRUE* to create this plot.

```{r, out.width='100%', fig.height=7, eval=FALSE}
ggpairs(diamonds_sample,
        lower = list(combo = wrap("facethist", bins = 10)))
```

ggpairs plot of all the highly correlated variables in the dataset. Set *eval* to *TRUE* to create this plot.

```{r, out.width='100%', fig.height=7, eval=FALSE}
ggpairs(diamonds_sample %>% select(carat, price:z),
        lower = list(combo = wrap("facethist", bins = 10)))
```

Histograms with the distribution of the target variable *price*.

```{r}
plot3 <- ggplot(diamonds_sample, aes(price)) +
  geom_histogram(bins = 20) +
  theme_minimal()
plot4 <- ggplot(diamonds_sample, aes(price)) +
  geom_histogram(bins = 20) +
  scale_x_log10() +
  theme_minimal()
grid.arrange(plot3,plot4,ncol=2)
```

# Variable transformation

Create a variable with the logarithm of the target variable.

```{r}
diamonds <- diamonds %>%
  mutate(log_price = log10(price))
```


# Data split

Split between the train set and the test set.

```{r}
set.seed(2022)
diamonds_split <- initial_split(diamonds, prop = 0.8, strata = log_price)
```

# Recipes

Create recipe.

```{r}
initial_recipe <- training(diamonds_split) %>%
  recipe(log_price ~ .) %>%
  step_rm(price) %>%
  step_poly(carat, degree = 2) %>%
  step_dummy(all_nominal(), one_hot = TRUE) %>%
  step_corr(all_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors())

initial_recipe
```

Glimpse into recipe.

```{r}
initial_recipe %>% prep() %>% juice() %>% glimpse()
```

# Creating models

## Regression based models

Show engines of regression based models:

```{r}
show_engines("linear_reg")
```

linear regression: lm

```{r}
lm <- linear_reg(mode = "regression") %>%
  set_engine("lm")
```

Regularized linear regression: glmnet

```{r}
glmnet <- linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet")
```

## Tree based models

Show engines of random forest based models:

```{r}
show_engines("rand_forest") %>%
  filter(mode == "regression")
```

Random forest model with the ranger engine:

```{r}
rf_ranger <- rand_forest(mode = "regression", mtry = tune(), trees = tune()) %>%
  set_engine("ranger")
```

# Testing the models

## Metric set and folds

```{r}
metric_diamonds <- metric_set(rsq, mae, rmse)
```

```{r}
folds <- vfold_cv(training(diamonds_split), v = 5, repeats = 2)
```

## Cross validating our linear regression model

```{r}
wf_lm <- workflow() %>%
  add_recipe(initial_recipe) %>%
  add_model(lm)
```

```{r, message=FALSE}
fit_resamples(wf_lm, 
              folds,
              metrics = metric_diamonds) %>%
  collect_metrics()
```

## Cross validating and tuning our regularized linear regression model

Creating the workflow using the recipe and the glmnet model.

```{r}
wf_glmnet <- workflow() %>%
  add_recipe(initial_recipe) %>%
  add_model(glmnet)
```

Hyperparameter tuning our regularized general linear regression model.

```{r, message=FALSE}
glmn_set <- parameters(penalty(range = c(-5,1), trans = log10_trans()), mixture())

glmn_grid <- grid_regular(glmn_set, levels = c(7, 5))

ctrl <- control_grid(save_pred = TRUE, verbose = TRUE)

glm_tune <- tune_grid(wf_glmnet,
                      resamples = folds,
                      grid = glmn_grid,
                      metrics = metric_diamonds,
                      control = ctrl)
```
Top 10 models based on hyper parameter tuning.

```{r}
glm_tune %>% 
  collect_metrics() %>%
  filter(.metric == "rsq") %>%
  arrange(-mean) %>%
  head(10)
```

Settings of the best model extracted from hyper parameter tuning.

```{r}
best_glmn <- select_best(glm_tune, metric = "rsq")
best_glmn
```

## Cross validating and tuning our random forest 

Creating the workflow using the recipe and the random forest model.

```{r}
wf_rf_ranger <- workflow() %>%
  add_recipe(initial_recipe) %>%
  add_model(rf_ranger)
```

```{r}
rf_grid <- expand.grid(mtry = c(1, 5, 10), trees = c(5, 10, 15))
```

```{r}
rf_tune <- tune_grid(object = rf_ranger, 
                     preprocessor = initial_recipe, 
                     resamples = folds, 
                     grid = rf_grid, 
                     metrics = metric_set(rsq, rmse, mae))
show_best(rf_tune, metric = "rsq")
```

