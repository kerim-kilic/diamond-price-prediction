---
title: "Diamond price prediction"
author: "Kerim KiliÃ§"
subtitle: Supervised Machined Learning
output:
  html_document:
    df_print: paged
    toc: true
    toc_depth: 2
    number_sections: true
    toc_float: true
---

# Libraries

The following three libraries are used in this R markdown file.

```{r setup, message=FALSE}
library(tidymodels)
library(GGally)
library(gridExtra)
```

# Dataset descriptive analytics

Glimpse into the variables of the *diamonds* dataset.

```{r}
diamonds %>% glimpse()
```

Take a smaller sample from dataset to reduce time to train the model.

```{r}
diamonds_sample <- diamonds %>% slice_sample(prop = 0.1)
```

ggpairs plot of all the variables and their correlation in the dataset. Set *eval* to *TRUE* to create this plot.

```{r, out.width='100%', fig.height=7, eval=FALSE}
ggpairs(diamonds_sample, lower = list(combo = wrap("facethist", bins = 10)))
```

ggpairs plot of all the highly correlated variables in the dataset. Set *eval* to *TRUE* to create this plot.

```{r, out.width='100%', fig.height=7, eval=FALSE}
ggpairs(diamonds_sample %>% select(carat, price:z), 
        lower = list(combo = wrap("facethist", bins = 10)))
```

Histograms with the distribution of the target variable *price*.

```{r}
plot1 <- ggplot(diamonds_sample, aes(price)) +
  geom_histogram(bins = 20) +
  theme_minimal()
plot2 <- ggplot(diamonds_sample, aes(price)) +
  geom_histogram(bins = 20) +
  scale_x_log10() +
  theme_minimal()
grid.arrange(plot1, plot2, ncol = 2)
```

# Variable transformation and splitting the data

Setting the seed for reproducibility. Creating a variable with the logarithm of the target variable *price* and splitting the data into a training and test set.

```{r}
set.seed(2022)
diamonds <- diamonds %>% 
  mutate(log_price = log10(price))
diamonds_split <- initial_split(diamonds, prop = 0.8, strata = log_price)
```

# Recipes

Create recipe using the logarithm of the target variable *price* and remove the reguler *price* variable. Introducing a quadratic term for the *carat*. Encode all dummy predictors to one hot encoding, remove all highly correlated predictors and predictors with very low variance. And finally center and scale all the numeric predictors.

```{r}
initial_recipe <- training(diamonds_split) %>%
  recipe(log_price ~ .) %>%
  step_rm(price) %>%
  step_poly(carat, degree = 2) %>%
  step_dummy(all_nominal(), one_hot = TRUE) %>%
  step_corr(all_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors())

initial_recipe
```

Glimpse into all the variables of the recipe.

```{r}
initial_recipe %>% prep() %>% juice() %>% glimpse()
```

# Creating models

## Regression based models

Show the different engines of linear regression based models:

```{r}
show_engines("linear_reg")
```

Defining a linear regression model using the lm engine:

```{r}
lm <- linear_reg(mode = "regression") %>%
  set_engine("lm")
```

Defining a generalized linear model using the glmnet engine, penalty and mixture are to be tuned when testing the model:

```{r}
glmnet <- linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet")
```

## Tree based models

Show engines of random forest based models:

```{r}
show_engines("rand_forest") %>%
  filter(mode == "regression")
```

Defining the random forest model using the ranger engine:

```{r}
rf_ranger <- rand_forest(mode = "regression", mtry = tune(), trees = tune()) %>%
  set_engine("ranger")
```

## Neural network using keras

Defining the linear regression model using the Keras engine:

```{r}
keras <- linear_reg(mode = "regression") %>% 
  set_engine("keras")
```

# Testing the models

## Metric set and folds

Defining the metrics for linear regression model performance: R-Squared, Mean Absolute Error, and Root Mean Square Error.

```{r}
metric_diamonds <- metric_set(rsq, mae, rmse)
```

Defining the number of folds for the cross validation to 4 and keeping repeats at 1 to minimize the time to train each model.

```{r}
folds <- vfold_cv(training(diamonds_split), v = 4)
```

## Cross validating our linear regression model (lm)

Creating the workflow for our linear regression model using the lm engine.

```{r}
wf_lm <- workflow() %>%
  add_recipe(initial_recipe) %>%
  add_model(lm)
```

Testing the linear regression model using cross validation.

```{r, message=FALSE}
lm_result <- fit_resamples(wf_lm, 
                           folds,
                           metrics = metric_diamonds) %>%
  collect_metrics() 

lm_result
```

## Cross validating and tuning our regularized linear regression model (glmnet)

Creating the workflow using the recipe and the glmnet model.

```{r}
wf_glmnet <- workflow() %>%
  add_recipe(initial_recipe) %>%
  add_model(glmnet)
```

Hyperparameter tuning our regularized general linear regression model.

```{r, message=FALSE}
glmn_set <- parameters(penalty(range = c(-5,1), trans = log10_trans()), mixture())

glmn_grid <- grid_regular(glmn_set, levels = c(7, 5))

ctrl <- control_grid(save_pred = TRUE, verbose = TRUE)

glm_tune <- tune_grid(wf_glmnet,
                      resamples = folds,
                      grid = glmn_grid,
                      metrics = metric_diamonds,
                      control = ctrl)
```

Top 10 models based on hyper parameter tuning the *penalty* and *mixture* parameters.

```{r}
glm_result <- glm_tune %>% 
  collect_metrics() %>%
  filter(.metric == "rsq") %>%
  arrange(-mean) %>%
  head(10)
```

Settings of the best model extracted from hyper parameter tuning.

```{r}
best_glmn <- select_best(glm_tune, metric = "rsq")
best_glmn
```

## Cross validating and tuning our random forest (ranger)

Creating the workflow using the recipe and the random forest model with the ranger engine.

```{r}
wf_rf_ranger <- workflow() %>%
  add_recipe(initial_recipe) %>%
  add_model(rf_ranger)
```

Setting the grid for hyper parameter tuning for the *mtry* and number of *trees*.

```{r}
rf_grid <- expand.grid(mtry = c(1, 5, 10), trees = c(5, 10, 15))
```

Cross validating and hyper parameter tuning the randomforest model.

```{r}
rf_tune <- tune_grid(object = rf_ranger, 
                     preprocessor = initial_recipe, 
                     resamples = folds, 
                     grid = rf_grid, 
                     metrics = metric_set(rsq, rmse, mae))
rf_result <- show_best(rf_tune, metric = "rsq")

rf_result
```

## Cross validating and tuning our deep learning model (keras)

Create a workflow for the keras model

```{r}
wf_keras <- workflow() %>%
  add_recipe(initial_recipe) %>%
  add_model(keras)
```

Test the model with cross validation.

```{r, message=FALSE}
keras_result <- fit_resamples(wf_keras, 
                              folds,
                              metrics = metric_diamonds) %>%
  collect_metrics()

keras_result
```

# Comparing model performance

Let's compare the results of the performance of the four trained models.

```{r}
lm_result_final <- lm_result %>%
  mutate(model_engine = c("lm")) %>%
  filter(.metric == "rsq") %>%
  head(1)

glmn_result_final <- glm_result %>%
  mutate(model_engine = c("glmnet")) %>%
  filter(.metric == "rsq") %>%
  select(.metric:model_engine) %>%
  head(1)

rf_result_final <- rf_result %>%
  mutate(model_engine = c("ranger")) %>%
  filter(.metric == "rsq") %>%
  select(.metric:model_engine) %>%
  head(1)

keras_result_final <- keras_result %>%
  mutate(model_engine = c("keras")) %>%
  filter(.metric == "rsq") %>%
  head(1)

final_results <- rbind(lm_result_final, glmn_result_final, rf_result_final, keras_result_final) %>%
  select(.metric, mean, n, std_err, .config, model_engine) %>%
  mutate(mean = round(mean,3),
         std_err = round(std_err,5)) %>%
  arrange(-mean)

final_results
```

Based on the test results from cross validation and hyper parameter tuning it turns out the `r final_results[1,6]` engine has the best performance with an R-Squared value of `r final_results[1,2]`. This is the model we will fit to the entire training data set and compare the model performance on unseen data in the test set.

